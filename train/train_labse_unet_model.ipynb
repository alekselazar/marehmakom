{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b65b61",
      "metadata": {
        "id": "56b65b61"
      },
      "outputs": [],
      "source": [
        "import os, json, math, time, random, platform, argparse\n",
        "from typing import Dict, Iterator, List, Optional\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "try:\n",
        "    from torch.utils.data import BufferedShuffleDataset  # optional, not required\n",
        "    HAS_BUF_SHUFFLE = True\n",
        "except Exception:\n",
        "    HAS_BUF_SHUFFLE = False\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer, PreTrainedTokenizerFast, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1dc6669",
      "metadata": {
        "id": "b1dc6669"
      },
      "outputs": [],
      "source": [
        "def load_local_tokenizer(path: str):\n",
        "    tok = PreTrainedTokenizerFast(tokenizer_file=path)\n",
        "    # Ensure specials\n",
        "    if tok.cls_token is None: tok.cls_token = \"[CLS]\"\n",
        "    if tok.sep_token is None: tok.sep_token = \"[SEP]\"\n",
        "    if tok.pad_token is None: tok.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "    return tok\n",
        "\n",
        "import json, torch, random\n",
        "from typing import Dict, List, Optional, Iterator\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "\n",
        "class InMemoryJsonlRows(Dataset):\n",
        "    \"\"\"\n",
        "    Loads a whole JSONL into RAM on init.\n",
        "    Each row is expected to have:\n",
        "      {\"query_tokenized\":[...], \"target_tokenized\":[...], \"target_mask\":[...]}\n",
        "    \"\"\"\n",
        "    def __init__(self, path: str):\n",
        "        self.data: List[Dict] = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                self.data.append(json.loads(line))\n",
        "            print('Loaded')\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict:\n",
        "        return self.data[idx]\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def pad_1d(seqs: List[torch.Tensor], pad_val: int) -> torch.Tensor:\n",
        "    max_len = max(x.size(0) for x in seqs)\n",
        "    out = []\n",
        "    for x in seqs:\n",
        "        if x.size(0) < max_len:\n",
        "            pad = torch.full((max_len - x.size(0),), pad_val, dtype=x.dtype)\n",
        "            x = torch.cat([x, pad], dim=0)\n",
        "        out.append(x)\n",
        "    return torch.stack(out, dim=0)\n",
        "\n",
        "def make_collate_two_stream(cls_id: int, sep_id: int, pad_id: int,\n",
        "                            max_query_len: Optional[int] = 128,\n",
        "                            max_target_len: Optional[int] = 480):\n",
        "    \"\"\"\n",
        "    Returns a collate_fn that:\n",
        "      - truncates q/t to max lengths\n",
        "      - inserts [CLS] ... [SEP]\n",
        "      - aligns target_mask to target and makes labels (0 on CLS/SEP)\n",
        "      - pads everything to batch max\n",
        "    \"\"\"\n",
        "    def collate(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        q_ids_list, q_attn_list = [], []\n",
        "        t_ids_list, t_attn_list = [], []\n",
        "        labels_list = []\n",
        "\n",
        "        for ex in batch:\n",
        "            q = ex[\"query_tokenized\"]\n",
        "            t = ex[\"target_tokenized\"]\n",
        "            m = ex[\"target_mask\"]\n",
        "\n",
        "            # truncate\n",
        "            if max_query_len: q = q[:max_query_len]\n",
        "            if max_target_len: t = t[:max_target_len]\n",
        "\n",
        "            # align mask length to (possibly truncated) target\n",
        "            if len(m) != len(ex[\"target_tokenized\"]):\n",
        "                # if your data guarantees equality you can drop this guard\n",
        "                if len(m) > len(ex[\"target_tokenized\"]):\n",
        "                    m = m[:len(ex[\"target_tokenized\"])]\n",
        "                else:\n",
        "                    m = m + [0] * (len(ex[\"target_tokenized\"]) - len(m))\n",
        "            m = m[:len(t)]\n",
        "            if len(m) < len(t):\n",
        "                m = m + [0] * (len(t) - len(m))\n",
        "\n",
        "            # insert specials\n",
        "            q_ids = [cls_id] + q + [sep_id]\n",
        "            t_ids = [cls_id] + t + [sep_id]\n",
        "\n",
        "            # attention masks (1 where real tokens)\n",
        "            q_attn = [1] * len(q_ids)\n",
        "            t_attn = [1] * len(t_ids)\n",
        "\n",
        "            # labels over target: 0 on CLS/SEP\n",
        "            labels = [0.0] * len(t_ids)\n",
        "            for i, bit in enumerate(m):\n",
        "                labels[1 + i] = float(bit)\n",
        "\n",
        "            # to tensors\n",
        "            q_ids_list.append(torch.tensor(q_ids, dtype=torch.long))\n",
        "            q_attn_list.append(torch.tensor(q_attn, dtype=torch.long))\n",
        "            t_ids_list.append(torch.tensor(t_ids, dtype=torch.long))\n",
        "            t_attn_list.append(torch.tensor(t_attn, dtype=torch.long))\n",
        "            labels_list.append(torch.tensor(labels, dtype=torch.float))\n",
        "\n",
        "        # pad to batch max\n",
        "        q_ids  = pad_1d(q_ids_list, pad_id)\n",
        "        q_attn = pad_1d(q_attn_list, 0)\n",
        "        t_ids  = pad_1d(t_ids_list, pad_id)\n",
        "        t_attn = pad_1d(t_attn_list, 0)\n",
        "        labels = pad_1d(labels_list, 0)\n",
        "\n",
        "        valid_mask = (t_attn == 1).float()  # ignores CLS/SEP + padding\n",
        "        return {\n",
        "            \"query_input_ids\": q_ids,\n",
        "            \"query_attention_mask\": q_attn,\n",
        "            \"target_input_ids\": t_ids,\n",
        "            \"target_attention_mask\": t_attn,\n",
        "            \"labels_full\": labels,\n",
        "            \"valid_mask\": valid_mask\n",
        "        }\n",
        "    return collate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eeadd51",
      "metadata": {
        "id": "3eeadd51"
      },
      "outputs": [],
      "source": [
        "class ConvBlock1D(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, k=3, p=None):\n",
        "        super().__init__()\n",
        "        if p is None: p = k // 2\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv1d(in_ch, out_ch, k, padding=p),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(out_ch, out_ch, k, padding=p),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class UNet1DHead(nn.Module):\n",
        "    def __init__(self, hidden=768, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.enc1 = ConvBlock1D(hidden, 256)\n",
        "        self.pool1 = nn.MaxPool1d(2, ceil_mode=True)\n",
        "        self.enc2 = ConvBlock1D(256, 512)\n",
        "        self.pool2 = nn.MaxPool1d(2, ceil_mode=True)\n",
        "        self.bottleneck = ConvBlock1D(512, 1024)\n",
        "        self.up2 = nn.ConvTranspose1d(1024, 512, 2, stride=2)\n",
        "        self.dec2 = ConvBlock1D(1024, 512)\n",
        "        self.up1 = nn.ConvTranspose1d(512, 256, 2, stride=2)\n",
        "        self.dec1 = ConvBlock1D(512, 256)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Conv1d(256, 1, 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _crop_or_pad(x, T_target):\n",
        "        B, C, T = x.shape\n",
        "        if T == T_target: return x\n",
        "        if T > T_target:\n",
        "            start = (T - T_target) // 2\n",
        "            return x[:, :, start:start+T_target]\n",
        "        pad = T_target - T\n",
        "        left = pad // 2; right = pad - left\n",
        "        return nn.functional.pad(x, (left, right))\n",
        "\n",
        "    def forward(self, hs_tgt):  # [B,T,H]\n",
        "        T_orig = hs_tgt.size(1)\n",
        "        x = hs_tgt.permute(0, 2, 1)      # [B,H,T]\n",
        "        e1 = self.enc1(x)                # [B,256,T]\n",
        "        p1 = self.pool1(e1)              # ~T/2\n",
        "        e2 = self.enc2(p1)               # [B,512,~T/2]\n",
        "        p2 = self.pool2(e2)              # ~T/4\n",
        "        b  = self.bottleneck(p2)         # [B,1024,~T/4]\n",
        "        u2 = self.up2(b)                 # ~T/2\n",
        "        e2c = self._crop_or_pad(e2, u2.size(-1))\n",
        "        d2 = self.dec2(torch.cat([u2, e2c], dim=1))\n",
        "        u1 = self.up1(d2)                # ~T\n",
        "        e1c = self._crop_or_pad(e1, u1.size(-1))\n",
        "        d1 = self.dec1(torch.cat([u1, e1c], dim=1))\n",
        "        d1 = self.drop(d1)\n",
        "        logits = self.classifier(d1).squeeze(1)  # [B,~T]\n",
        "        if logits.size(1) != T_orig:\n",
        "            logits = self._crop_or_pad(logits.unsqueeze(1), T_orig).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "class FiLMConditioner(nn.Module):\n",
        "    def __init__(self, hidden=768):\n",
        "        super().__init__()\n",
        "        self.gamma = nn.Linear(hidden, hidden)\n",
        "        self.beta  = nn.Linear(hidden, hidden)\n",
        "    def forward(self, hs_tgt, q_vec):  # hs_tgt [B,T,H], q_vec [B,H]\n",
        "        gamma = self.gamma(q_vec).unsqueeze(1)  # [B,1,H]\n",
        "        beta  = self.beta(q_vec).unsqueeze(1)   # [B,1,H]\n",
        "        return hs_tgt * (1.0 + gamma) + beta\n",
        "\n",
        "class LabseTwoStreamUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Shared frozen LaBSE encodes query and target separately.\n",
        "    Query pooled -> FiLM conditioning of target features -> U-Net head -> per-target-token logits.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_model: str, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(base_model)\n",
        "        for p in self.encoder.parameters(): p.requires_grad = False\n",
        "        for p in self.encoder.encoder.layer[-4:].parameters(): p.requires_grad = True\n",
        "        hidden = self.encoder.config.hidden_size\n",
        "        self.conditioner = FiLMConditioner(hidden=hidden)\n",
        "        self.head = UNet1DHead(hidden=hidden, dropout=dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def mean_pool(hs, attn):  # hs [B,T,H], attn [B,T]\n",
        "        mask = attn.unsqueeze(-1).float()\n",
        "        num = (hs * mask).sum(dim=1)\n",
        "        den = mask.sum(dim=1).clamp_min(1.0)\n",
        "        return num / den\n",
        "\n",
        "    def forward(self, q_ids, q_attn, t_ids, t_attn):\n",
        "        q_out = self.encoder(input_ids=q_ids, attention_mask=q_attn)\n",
        "        q_vec = self.mean_pool(q_out.last_hidden_state, q_attn)  # [B,H]\n",
        "\n",
        "        t_out = self.encoder(input_ids=t_ids, attention_mask=t_attn)\n",
        "        t_hs  = t_out.last_hidden_state                           # [B,T,H]\n",
        "\n",
        "        t_cond = self.conditioner(t_hs, q_vec)                    # [B,T,H]\n",
        "        logits = self.head(t_cond)                                # [B,T]\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d674ae28",
      "metadata": {
        "id": "d674ae28"
      },
      "outputs": [],
      "source": [
        "def _bin_to_spans(bin_arr):\n",
        "    \"\"\"\n",
        "    Convert a 0/1 iterable into spans over indices [start, end) where 1's are contiguous.\n",
        "    Example: [0,1,1,0,1] -> [(1,3), (4,5)]\n",
        "    \"\"\"\n",
        "    spans = []\n",
        "    in_run = False\n",
        "    start = 0\n",
        "    for i, v in enumerate(bin_arr):\n",
        "        if v and not in_run:\n",
        "            in_run = True\n",
        "            start = i\n",
        "        elif not v and in_run:\n",
        "            in_run = False\n",
        "            spans.append((start, i))\n",
        "    if in_run:\n",
        "        spans.append((start, len(bin_arr)))\n",
        "    return spans\n",
        "\n",
        "def _exact_span_prf1(pred_spans, gold_spans):\n",
        "    \"\"\"\n",
        "    Exact boundary match. Count TP when a predicted (s,e) is in gold exactly.\n",
        "    \"\"\"\n",
        "    pred_set = set(pred_spans)\n",
        "    gold_set = set(gold_spans)\n",
        "    tp = len(pred_set & gold_set)\n",
        "    fp = len(pred_set - gold_set)\n",
        "    fn = len(gold_set - pred_set)\n",
        "    prec = tp / (tp + fp + 1e-9)\n",
        "    rec  = tp / (tp + fn + 1e-9)\n",
        "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
        "    return tp, fp, fn, prec, rec, f1\n",
        "\n",
        "def _span_metrics_from_logits_row(logits_row, labels_row, t_attn_row, thr=0.5, smooth_k=0):\n",
        "    \"\"\"\n",
        "    Compute span-level stats for a single sample in the batch.\n",
        "    We:\n",
        "      - restrict to real target tokens (exclude target CLS at pos 0 and SEP at last)\n",
        "      - threshold probs -> binary\n",
        "      - optional 1D median smoothing (smooth_k must be odd >=3; 0 = off)\n",
        "      - convert to spans and compare to gold spans\n",
        "    \"\"\"\n",
        "    import torch\n",
        "\n",
        "    # length of target stream (including CLS/SEP)\n",
        "    L = int(t_attn_row.sum().item())\n",
        "    if L < 3:\n",
        "        # not enough tokens to form text (CLS + SEP only)\n",
        "        return 0, 0, 0, 0.0, 0.0, 0.0\n",
        "\n",
        "    # Slice to TEXT ONLY region: positions [1 .. L-2]\n",
        "    probs = torch.sigmoid(logits_row[1:L-1])\n",
        "    gold  = labels_row[1:L-1]  # already 0/1 there\n",
        "\n",
        "    if smooth_k and smooth_k >= 3 and (smooth_k % 2 == 1):\n",
        "        # simple median filter (no external deps)\n",
        "        pad = smooth_k // 2\n",
        "        # replicate-pad\n",
        "        padded = torch.nn.functional.pad(probs.unsqueeze(0).unsqueeze(0), (pad, pad), mode='replicate')[0,0]\n",
        "        windows = padded.unfold(0, smooth_k, 1)\n",
        "        probs = windows.median(dim=1).values\n",
        "\n",
        "    pred_bin = (probs >= thr).to(torch.int).tolist()\n",
        "    gold_bin = gold.to(torch.int).tolist()\n",
        "\n",
        "    pred_spans = _bin_to_spans(pred_bin)\n",
        "    gold_spans = _bin_to_spans(gold_bin)\n",
        "\n",
        "    return _exact_span_prf1(pred_spans, gold_spans)\n",
        "\n",
        "def span_eval_batch(logits, labels_full, target_attention_mask, thr=0.5, smooth_k=0):\n",
        "    \"\"\"\n",
        "    Aggregate span-level metrics over a batch.\n",
        "    Returns (prec, rec, f1, tp, fp, fn).\n",
        "    \"\"\"\n",
        "    B = logits.size(0)\n",
        "    tp=fp=fn=0\n",
        "    for i in range(B):\n",
        "        _tp,_fp,_fn, *_ = _span_metrics_from_logits_row(\n",
        "            logits[i], labels_full[i], target_attention_mask[i], thr=thr, smooth_k=smooth_k\n",
        "        )\n",
        "        tp += _tp; fp += _fp; fn += _fn\n",
        "    prec = tp / (tp + fp + 1e-9)\n",
        "    rec  = tp / (tp + fn + 1e-9)\n",
        "    f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
        "    return prec, rec, f1, tp, fp, fn\n",
        "\n",
        "def token_f1_precision_recall(logits, labels_full, valid_mask, thr=0.5):\n",
        "    with torch.no_grad():\n",
        "        probs = torch.sigmoid(logits)\n",
        "        preds = (probs >= thr).float()\n",
        "        y_true = labels_full * valid_mask\n",
        "        y_pred = preds * valid_mask\n",
        "        tp = (y_true * y_pred).sum().item()\n",
        "        fp = ((1 - y_true) * y_pred).sum().item()\n",
        "        fn = (y_true * (1 - y_pred)).sum().item()\n",
        "        prec = tp / (tp + fp + 1e-9)\n",
        "        rec  = tp / (tp + fn + 1e-9)\n",
        "        f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
        "        return f1, prec, rec\n",
        "\n",
        "def bench_throughput(loader, model, steps=50):\n",
        "    device = next(model.parameters()).device\n",
        "    it = iter(loader)\n",
        "    # warmup a few\n",
        "    for _ in range(3):\n",
        "        try:\n",
        "            b = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(loader); b = next(it)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            _ = model(b[\"query_input_ids\"].to(device),\n",
        "                      b[\"query_attention_mask\"].to(device),\n",
        "                      b[\"target_input_ids\"].to(device),\n",
        "                      b[\"target_attention_mask\"].to(device))\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    batches = 0\n",
        "    tokens = 0\n",
        "    for _ in range(steps):\n",
        "        try:\n",
        "            b = next(it)\n",
        "        except StopIteration:\n",
        "            it = iter(loader); b = next(it)\n",
        "        B = b[\"target_input_ids\"].shape[0]\n",
        "        Tq = b[\"query_input_ids\"].shape[1]\n",
        "        Tt = b[\"target_input_ids\"].shape[1]\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            _ = model(b[\"query_input_ids\"].to(device),\n",
        "                      b[\"query_attention_mask\"].to(device),\n",
        "                      b[\"target_input_ids\"].to(device),\n",
        "                      b[\"target_attention_mask\"].to(device))\n",
        "        batches += 1\n",
        "        tokens  += B * (Tq + Tt)\n",
        "    torch.cuda.synchronize()\n",
        "    dt = time.time() - t0\n",
        "    return tokens / dt, batches / dt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa3aa84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830,
          "referenced_widgets": [
            "6554cc5f3f1f401cbb077fdf501bf522",
            "8f17b2ffff734798ac0f25c8d14dfee6",
            "c0755bec003045ce95218e92070e6368",
            "76ecf2558dd64a47a8ce5f22035037d1",
            "e3faf969a3bc4adca65e010909e77985",
            "962c902cd43744688134bfd784afe660",
            "1cb51e30144146a5b98a76fd9431ddf1",
            "3588fbac6b004053a396c2dc809622ce",
            "bbae19bc696f407db9d93ddfd3d13830",
            "74272a2bd2c44c9cbe12ceff039379c0",
            "6d593f6a97e0491aa98098844983314e",
            "7d31916cde584a4fb209ac452ea788ef",
            "14a86a4f26e9472a9711c9e153c59d14",
            "525b1f6ad2234d75835beb7f68cdc838",
            "2e16f419b0874bb4bb437a64980e41b9",
            "41fb61266e2a434eb5a6c2796badb992",
            "59382b9f5810476c969c09cb3b4c0430",
            "c6f3bfc2294e46cf98a6416689622acd",
            "4b5bc5a7d5214c2e8a5abdb9666653b6",
            "402ba03c1ad244eaa8dcda2bf40b0828",
            "2265c608133b49d6be5ae3c049493aeb",
            "a954a8d836b84da1a4c905ee63985c3c"
          ]
        },
        "id": "6fa3aa84",
        "outputId": "fbc6f0cb-3f8a-466f-e11f-62cbfd1f4132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded\n",
            "Loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6554cc5f3f1f401cbb077fdf501bf522",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/804 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d31916cde584a4fb209ac452ea788ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BENCH] ~128505 tokens/s, 1.34 batches/s on this setup.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15 [train]: 100%|██████████| 1000/1000 [23:44<00:00,  1.42s/it, F1=0.6682, P=0.8802, R=0.5386, loss=0.0460]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.6728 (P 0.7517 R 0.6147)  | span-F1 0.1912 (P 0.2032 R 0.1805)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/15 [train]: 100%|██████████| 1000/1000 [23:38<00:00,  1.42s/it, F1=0.8251, P=0.8365, R=0.8141, loss=0.0250]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.8178 (P 0.8674 R 0.7776)  | span-F1 0.5107 (P 0.5239 R 0.4982)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/15 [train]: 100%|██████████| 1000/1000 [23:39<00:00,  1.42s/it, F1=0.9067, P=0.9452, R=0.8712, loss=0.0162]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.8583 (P 0.9096 R 0.8167)  | span-F1 0.6096 (P 0.6369 R 0.5845)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/15 [train]: 100%|██████████| 1000/1000 [23:40<00:00,  1.42s/it, F1=0.8378, P=0.8389, R=0.8366, loss=0.0205]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.8761 (P 0.8760 R 0.8798)  | span-F1 0.6433 (P 0.6455 R 0.6410)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/15 [train]: 100%|██████████| 1000/1000 [23:35<00:00,  1.42s/it, F1=0.9037, P=0.9426, R=0.8679, loss=0.0137]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.8832 (P 0.8836 R 0.8861)  | span-F1 0.6681 (P 0.6654 R 0.6708)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/15 [train]: 100%|██████████| 1000/1000 [23:36<00:00,  1.42s/it, F1=0.9160, P=0.9398, R=0.8935, loss=0.0093]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.8980 (P 0.9221 R 0.8780)  | span-F1 0.7125 (P 0.7276 R 0.6981)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/15 [train]: 100%|██████████| 1000/1000 [23:34<00:00,  1.41s/it, F1=0.9270, P=0.9538, R=0.9016, loss=0.0087]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.8973 (P 0.9070 R 0.8909)  | span-F1 0.7151 (P 0.7215 R 0.7089)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/15 [train]: 100%|██████████| 1000/1000 [23:35<00:00,  1.42s/it, F1=0.9499, P=0.9641, R=0.9361, loss=0.0067]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.9060 (P 0.9278 R 0.8880)  | span-F1 0.7318 (P 0.7433 R 0.7207)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/15 [train]: 100%|██████████| 1000/1000 [23:37<00:00,  1.42s/it, F1=0.9375, P=0.9334, R=0.9417, loss=0.0086]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.9030 (P 0.9012 R 0.9074)  | span-F1 0.7277 (P 0.7205 R 0.7350)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/15 [train]: 100%|██████████| 1000/1000 [23:33<00:00,  1.41s/it, F1=0.9760, P=0.9797, R=0.9723, loss=0.0047]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.9092 (P 0.9297 R 0.8923)  | span-F1 0.7460 (P 0.7550 R 0.7372)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/15 [train]: 100%|██████████| 1000/1000 [23:40<00:00,  1.42s/it, F1=0.9625, P=0.9778, R=0.9476, loss=0.0055]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.9131 (P 0.9211 R 0.9076)  | span-F1 0.7518 (P 0.7532 R 0.7504)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/15 [train]: 100%|██████████| 1000/1000 [23:28<00:00,  1.41s/it, F1=0.9657, P=0.9947, R=0.9383, loss=0.0036]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.9125 (P 0.9370 R 0.8920)  | span-F1 0.7593 (P 0.7726 R 0.7465)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/15 [train]: 100%|██████████| 1000/1000 [23:36<00:00,  1.42s/it, F1=0.9842, P=0.9776, R=0.9909, loss=0.0021]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.9132 (P 0.9483 R 0.8833)  | span-F1 0.7600 (P 0.7820 R 0.7391)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/15 [train]: 100%|██████████| 1000/1000 [23:36<00:00,  1.42s/it, F1=0.9749, P=0.9784, R=0.9714, loss=0.0027]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.9160 (P 0.9277 R 0.9074)  | span-F1 0.7639 (P 0.7679 R 0.7600)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/15 [train]: 100%|██████████| 1000/1000 [23:33<00:00,  1.41s/it, F1=0.9728, P=0.9577, R=0.9884, loss=0.0033]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL] token-F1 0.9172 (P 0.9394 R 0.8987)  | span-F1 0.7720 (P 0.7829 R 0.7614)\n"
          ]
        }
      ],
      "source": [
        "tokenizer_path = '/content/drive/MyDrive/labse_tokenizer.json'\n",
        "model_name = \"sentence-transformers/LaBSE\"\n",
        "train_path = \"/content/drive/MyDrive/dataset-yeshibish-labse-train.jsonl\"\n",
        "val_path = \"/content/drive/MyDrive/dataset-yeshibish-labse-val.jsonl\"\n",
        "batch_size = 128\n",
        "max_epochs = 15\n",
        "lr = 2e-4\n",
        "weight_decay = 5e-4\n",
        "dropout = 0.3\n",
        "max_query_len = 480\n",
        "max_target_len = 480\n",
        "early_stop_patience = 4\n",
        "\n",
        "# OS-aware DataLoader settings\n",
        "is_windows = platform.system().lower().startswith(\"win\")\n",
        "num_workers = 0 if is_windows else 2\n",
        "pin_memory = not is_windows\n",
        "persistent = not is_windows\n",
        "\n",
        "tok = load_local_tokenizer(tokenizer_path)\n",
        "CLS, SEP, PAD = tok.cls_token_id, tok.sep_token_id, tok.pad_token_id\n",
        "\n",
        "train_ds = InMemoryJsonlRows(train_path)\n",
        "val_ds   = InMemoryJsonlRows(val_path)\n",
        "\n",
        "collate = make_collate_two_stream(CLS, SEP, PAD, max_query_len=max_query_len, max_target_len=max_target_len)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,                 # <-- free shuffle, no Drive I/O\n",
        "    collate_fn=collate,           # your collate_two_stream\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        "    persistent_workers=persistent\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=32,\n",
        "    shuffle=False,                # usually keep val deterministic\n",
        "    collate_fn=collate,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        "    persistent_workers=persistent\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = LabseTwoStreamUNet(base_model=model_name, dropout=dropout).to(device)\n",
        "# Trainable params only (FiLM + U-Net head)\n",
        "optim = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad],\n",
        "                            lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "# Estimate steps/epoch (streaming: approximate with a few batches)\n",
        "# If you know dataset size, you can set steps_per_epoch exactly.\n",
        "steps_per_epoch = 1000  # safe default; logging will still be useful\n",
        "\n",
        "total_steps = max_epochs * steps_per_epoch\n",
        "best_span_f1   = 0.0\n",
        "\n",
        "sched = get_linear_schedule_with_warmup(optim, num_warmup_steps=max(10, total_steps // 10),\n",
        "                                        num_training_steps=total_steps)\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "# quick throughput probe\n",
        "try:\n",
        "    tps, bps = bench_throughput(train_loader, model, steps=30)\n",
        "    print(f\"[BENCH] ~{tps:.0f} tokens/s, {bps:.2f} batches/s on this setup.\")\n",
        "except Exception as e:\n",
        "    print(f\"[BENCH] Skipped ({e})\")\n",
        "\n",
        "bad_epochs = 0\n",
        "train_iter = iter(train_loader)\n",
        "\n",
        "for epoch in range(1, max_epochs+1):\n",
        "    model.train()\n",
        "    with tqdm(total=steps_per_epoch, desc=f\"Epoch {epoch}/{max_epochs} [train]\", leave=True) as pbar:\n",
        "        for step_in_epoch in range(steps_per_epoch):\n",
        "            try:\n",
        "                batch = next(train_iter)\n",
        "            except StopIteration:\n",
        "                train_iter = iter(train_loader)  # restart\n",
        "                batch = next(train_iter)\n",
        "            q_ids  = batch[\"query_input_ids\"].to(device)\n",
        "            q_attn = batch[\"query_attention_mask\"].to(device)\n",
        "            t_ids  = batch[\"target_input_ids\"].to(device)\n",
        "            t_attn = batch[\"target_attention_mask\"].to(device)\n",
        "            labels = batch[\"labels_full\"].to(device)\n",
        "            valid  = batch[\"valid_mask\"].to(device)\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "                logits = model(q_ids, q_attn, t_ids, t_attn)  # [B,T]\n",
        "                # crop/pad logits to labels size (rare in practice)\n",
        "                if logits.size(1) != labels.size(1):\n",
        "                    T = labels.size(1)\n",
        "                    if logits.size(1) > T:\n",
        "                        logits = logits[:, :T]\n",
        "                    else:\n",
        "                        pad = torch.zeros((logits.size(0), T - logits.size(1)), device=logits.device)\n",
        "                        logits = torch.cat([logits, pad], dim=1)\n",
        "                loss_tok = bce(logits, labels) * valid\n",
        "                denom = valid.sum().clamp_min(1.0)\n",
        "                loss = loss_tok.sum() / denom\n",
        "\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optim)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(optim); scaler.update()\n",
        "            sched.step()\n",
        "\n",
        "            f1, p, r = token_f1_precision_recall(logits, labels, valid, thr=0.5)\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\", F1=f\"{f1:.4f}\", P=f\"{p:.4f}\", R=f\"{r:.4f}\")\n",
        "            pbar.update(1)\n",
        "\n",
        "\n",
        "    # Validation epoch (single pass)\n",
        "    model.eval()\n",
        "    val_loss_sum, val_tok_sum = 0.0, 0.0\n",
        "    agg_f1 = agg_p = agg_r = 0.0\n",
        "    nb = 0\n",
        "    span_tp = span_fp = span_fn = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            q_ids  = batch[\"query_input_ids\"].to(device)\n",
        "            q_attn = batch[\"query_attention_mask\"].to(device)\n",
        "            t_ids  = batch[\"target_input_ids\"].to(device)\n",
        "            t_attn = batch[\"target_attention_mask\"].to(device)\n",
        "            labels = batch[\"labels_full\"].to(device)\n",
        "            valid  = batch[\"valid_mask\"].to(device)\n",
        "\n",
        "            logits = model(q_ids, q_attn, t_ids, t_attn)\n",
        "            if logits.size(1) != labels.size(1):\n",
        "                T = labels.size(1)\n",
        "                if logits.size(1) > T:\n",
        "                    logits = logits[:, :T]\n",
        "                else:\n",
        "                    pad = torch.zeros((logits.size(0), T - logits.size(1)), device=logits.device)\n",
        "                    logits = torch.cat([logits, pad], dim=1)\n",
        "\n",
        "            loss_tok = bce(logits, labels) * valid\n",
        "            denom = valid.sum().clamp_min(1.0)\n",
        "            loss = loss_tok.sum() / denom\n",
        "            val_loss_sum += loss.item() * denom.item()\n",
        "            val_tok_sum  += denom.item()\n",
        "\n",
        "            f1, p, r = token_f1_precision_recall(logits, labels, valid, thr=0.5)\n",
        "            agg_f1 += f1; agg_p += p; agg_r += r; nb += 1\n",
        "\n",
        "            sp_prec, sp_rec, sp_f1, tp, fp, fn = span_eval_batch(\n",
        "            logits, labels, t_attn, thr=0.5, smooth_k=3  # try k=3; set 0 to disable\n",
        "            )\n",
        "            span_tp += tp; span_fp += fp; span_fn += fn\n",
        "\n",
        "    val_loss = val_loss_sum / max(1.0, val_tok_sum)\n",
        "    val_f1 = (agg_f1 / max(1, nb)) if nb else 0.0\n",
        "    val_p  = (agg_p  / max(1, nb)) if nb else 0.0\n",
        "    val_r  = (agg_r  / max(1, nb)) if nb else 0.0\n",
        "    span_prec = span_tp / (span_tp + span_fp + 1e-9)\n",
        "    span_rec  = span_tp / (span_tp + span_fn + 1e-9)\n",
        "    span_f1   = 2 * span_prec * span_rec / (span_prec + span_rec + 1e-9)\n",
        "\n",
        "    print(f\"[VAL] token-F1 {val_f1:.4f} (P {val_p:.4f} R {val_r:.4f})  \"\n",
        "        f\"| span-F1 {span_f1:.4f} (P {span_prec:.4f} R {span_rec:.4f})\")\n",
        "\n",
        "    # Early stopping + checkpoint by F1\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "    if span_f1 > best_span_f1 + 1e-4:\n",
        "        best_span_f1 = span_f1\n",
        "        bad_epochs = 0\n",
        "        state = {\n",
        "            \"epoch\": epoch,\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optim\": optim.state_dict(),\n",
        "            \"sched\": sched.state_dict(),\n",
        "            \"scaler\": scaler.state_dict(),\n",
        "            \"best_val_f1\": best_span_f1,\n",
        "            \"config\": { \"MODEL_NAME\": 'LaBse-Unet', \"dropout\": dropout }\n",
        "        }\n",
        "        torch.save(state, \"/content/drive/MyDrive/labse_unet_best_dropout_03_last_4_unfr.pt\")\n",
        "    else:\n",
        "        bad_epochs += 1\n",
        "        if bad_epochs >= early_stop_patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14a86a4f26e9472a9711c9e153c59d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59382b9f5810476c969c09cb3b4c0430",
            "placeholder": "​",
            "style": "IPY_MODEL_c6f3bfc2294e46cf98a6416689622acd",
            "value": "model.safetensors: 100%"
          }
        },
        "1cb51e30144146a5b98a76fd9431ddf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2265c608133b49d6be5ae3c049493aeb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e16f419b0874bb4bb437a64980e41b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2265c608133b49d6be5ae3c049493aeb",
            "placeholder": "​",
            "style": "IPY_MODEL_a954a8d836b84da1a4c905ee63985c3c",
            "value": " 1.88G/1.88G [00:16&lt;00:00, 357MB/s]"
          }
        },
        "3588fbac6b004053a396c2dc809622ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "402ba03c1ad244eaa8dcda2bf40b0828": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41fb61266e2a434eb5a6c2796badb992": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b5bc5a7d5214c2e8a5abdb9666653b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "525b1f6ad2234d75835beb7f68cdc838": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b5bc5a7d5214c2e8a5abdb9666653b6",
            "max": 1883734344,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_402ba03c1ad244eaa8dcda2bf40b0828",
            "value": 1883734344
          }
        },
        "59382b9f5810476c969c09cb3b4c0430": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6554cc5f3f1f401cbb077fdf501bf522": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f17b2ffff734798ac0f25c8d14dfee6",
              "IPY_MODEL_c0755bec003045ce95218e92070e6368",
              "IPY_MODEL_76ecf2558dd64a47a8ce5f22035037d1"
            ],
            "layout": "IPY_MODEL_e3faf969a3bc4adca65e010909e77985"
          }
        },
        "6d593f6a97e0491aa98098844983314e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74272a2bd2c44c9cbe12ceff039379c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76ecf2558dd64a47a8ce5f22035037d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74272a2bd2c44c9cbe12ceff039379c0",
            "placeholder": "​",
            "style": "IPY_MODEL_6d593f6a97e0491aa98098844983314e",
            "value": " 804/804 [00:00&lt;00:00, 97.0kB/s]"
          }
        },
        "7d31916cde584a4fb209ac452ea788ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14a86a4f26e9472a9711c9e153c59d14",
              "IPY_MODEL_525b1f6ad2234d75835beb7f68cdc838",
              "IPY_MODEL_2e16f419b0874bb4bb437a64980e41b9"
            ],
            "layout": "IPY_MODEL_41fb61266e2a434eb5a6c2796badb992"
          }
        },
        "8f17b2ffff734798ac0f25c8d14dfee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_962c902cd43744688134bfd784afe660",
            "placeholder": "​",
            "style": "IPY_MODEL_1cb51e30144146a5b98a76fd9431ddf1",
            "value": "config.json: 100%"
          }
        },
        "962c902cd43744688134bfd784afe660": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a954a8d836b84da1a4c905ee63985c3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbae19bc696f407db9d93ddfd3d13830": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0755bec003045ce95218e92070e6368": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3588fbac6b004053a396c2dc809622ce",
            "max": 804,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbae19bc696f407db9d93ddfd3d13830",
            "value": 804
          }
        },
        "c6f3bfc2294e46cf98a6416689622acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3faf969a3bc4adca65e010909e77985": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
